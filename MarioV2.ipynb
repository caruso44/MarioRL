{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuM8uip9Ilv6",
        "outputId": "bff55c67-cde0-4009-f6d4-2dce26f2213a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gym-super-mario-bros==7.4.0\n",
            "  Downloading gym_super_mario_bros-7.4.0-py3-none-any.whl (199 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 199.1/199.1 kB 3.3 MB/s eta 0:00:00\n",
            "Collecting nes-py>=8.1.4 (from gym-super-mario-bros==7.4.0)\n",
            "  Downloading nes_py-8.2.1.tar.gz (77 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 77.7/77.7 kB 4.4 MB/s eta 0:00:00\n",
            "  Preparing metadata (setup.py): started\n",
            "  Preparing metadata (setup.py): finished with status 'done'\n",
            "Requirement already satisfied: gym>=0.17.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.25.2)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (1.23.5)\n",
            "Collecting pyglet<=1.5.21,>=1.4.0 (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0)\n",
            "  Downloading pyglet-1.5.21-py3-none-any.whl (1.1 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 10.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: tqdm>=4.48.2 in /usr/local/lib/python3.10/dist-packages (from nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (4.66.1)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (2.2.1)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym>=0.17.2->nes-py>=8.1.4->gym-super-mario-bros==7.4.0) (0.0.8)\n",
            "Building wheels for collected packages: nes-py\n",
            "  Building wheel for nes-py (setup.py): started\n",
            "  Building wheel for nes-py (setup.py): finished with status 'done'\n",
            "  Created wheel for nes-py: filename=nes_py-8.2.1-cp310-cp310-linux_x86_64.whl size=535719 sha256=f985c73427fb7b67264bb4ffccdb57625b71a9bcc67433db41e126159c1a6305\n",
            "  Stored in directory: /root/.cache/pip/wheels/34/a7/d5/9aa14b15df740a53d41f702e4c795731b6c4da7925deb8476c\n",
            "Successfully built nes-py\n",
            "Installing collected packages: pyglet, nes-py, gym-super-mario-bros\n",
            "Successfully installed gym-super-mario-bros-7.4.0 nes-py-8.2.1 pyglet-1.5.21\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install gym-super-mario-bros==7.4.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UjXlyV4P05yT",
        "outputId": "9d417581-bc3a-4308-d9a1-5eb58aa09083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tensordict\n",
            "  Downloading tensordict-0.2.1-cp310-cp310-manylinux1_x86_64.whl (986 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 986.5/986.5 kB 7.0 MB/s eta 0:00:00\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from tensordict) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from tensordict) (1.23.5)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from tensordict) (2.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch->tensordict) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->tensordict) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->tensordict) (1.3.0)\n",
            "Installing collected packages: tensordict\n",
            "Successfully installed tensordict-0.2.1\n",
            "Collecting torchrl\n",
            "  Downloading torchrl-0.2.1-cp310-cp310-manylinux1_x86_64.whl (5.5 MB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 5.5/5.5 MB 14.5 MB/s eta 0:00:00\n",
            "Requirement already satisfied: torch>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.1.0+cu121)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchrl) (1.23.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from torchrl) (23.2)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from torchrl) (2.2.1)\n",
            "Requirement already satisfied: tensordict>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from torchrl) (0.2.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=2.1.0->torchrl) (2.1.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.1.0->torchrl) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=2.1.0->torchrl) (1.3.0)\n",
            "Installing collected packages: torchrl\n",
            "Successfully installed torchrl-0.2.1\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "pip install tensordict\n",
        "pip install torchrl"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyglet[glu]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vToaKraD1HlE",
        "outputId": "12fd831e-912a-4913-ce9f-cc9f02523c7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyglet[glu] in /usr/local/lib/python3.10/dist-packages (1.5.21)\n",
            "\u001b[33mWARNING: pyglet 1.5.21 does not provide the extra 'glu'\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%bash\n",
        "pip install stable_baselines3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fWPjq1bF0JHl",
        "outputId": "eb4e8144-abe7-4a95-c19c-0796233a5f2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting stable_baselines3\n",
            "  Downloading stable_baselines3-2.2.1-py3-none-any.whl (181 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 181.7/181.7 kB 5.2 MB/s eta 0:00:00\n",
            "Collecting gymnasium<0.30,>=0.28.1 (from stable_baselines3)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "     ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 953.9/953.9 kB 17.4 MB/s eta 0:00:00\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.23.5)\n",
            "Requirement already satisfied: torch>=1.13 in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.1.0+cu121)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (2.2.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from stable_baselines3) (3.7.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium<0.30,>=0.28.1->stable_baselines3) (4.5.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium<0.30,>=0.28.1->stable_baselines3)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.13.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13->stable_baselines3) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (3.1.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->stable_baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->stable_baselines3) (2023.3.post1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->stable_baselines3) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13->stable_baselines3) (2.1.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13->stable_baselines3) (1.3.0)\n",
            "Installing collected packages: farama-notifications, gymnasium, stable_baselines3\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 stable_baselines3-2.2.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -Uqq ipdb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iBpAqk7oe67H",
        "outputId": "4ed124b0-cad3-4669-da48-964bbbff1af3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.5/1.6 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmVtE8TNI4MS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torchvision import transforms as T\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from collections import deque\n",
        "import random, datetime, os, copy\n",
        "import gc\n",
        "import gym\n",
        "from gym.spaces import Box\n",
        "from gym.wrappers import FrameStack\n",
        "from tqdm import tqdm\n",
        "from collections import deque\n",
        "import gc\n",
        "from torchrl.data import TensorDictReplayBuffer, LazyMemmapStorage\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from tensordict import TensorDict\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "from stable_baselines3.common.policies import obs_as_tensor\n",
        "from stable_baselines3.common.evaluation import evaluate_policy\n",
        "import imageio\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import colors\n",
        "from gym import spaces\n",
        "\n",
        "# NES Emulator for OpenAI Gym\n",
        "from nes_py.wrappers import JoypadSpace\n",
        "\n",
        "# Super Mario environment for OpenAI Gym\n",
        "import gym_super_mario_bros\n",
        "import ipdb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb on"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQY-mJSxfYRe",
        "outputId": "6e1ab293-13e0-471c-c6db-74c197e88503"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned ON\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SMB():\n",
        "    '''\n",
        "    Wrapper function containing the processed environment and the loaded model\n",
        "    '''\n",
        "    def __init__(self, env, model):\n",
        "        self.env = env\n",
        "        self.model = model\n",
        "\n",
        "    def play(self, episodes=5, deterministic=False, render=True, return_eval=False):\n",
        "        for episode in range(1, episodes+1):\n",
        "            states = self.env.reset()\n",
        "            done = False\n",
        "            score = 0\n",
        "\n",
        "            if render == True:\n",
        "                while not done:\n",
        "                    self.env.render()\n",
        "                    action, _ = self.model.predict(states, deterministic=deterministic)\n",
        "                    states, reward, done, info = self.env.step(action)\n",
        "                    score += reward\n",
        "                    time.sleep(0.01)\n",
        "                print('Episode:{} Score:{}'.format(episode, score))\n",
        "            else:\n",
        "                while not done:\n",
        "                    action, _ = self.model.predict(states, deterministic=deterministic)\n",
        "                    states, reward, done, info = self.env.step(action)\n",
        "                    score += reward\n",
        "        if return_eval == True:\n",
        "            return score, info\n",
        "        else:\n",
        "            return\n",
        "\n",
        "    def evaluate(self, episodes=20, deterministic=False):\n",
        "        '''\n",
        "        returns rewards, steps (both have length [episodes])\n",
        "        '''\n",
        "        rewards, steps = evaluate_policy(self.model, self.env, n_eval_episodes=episodes,\n",
        "                                 deterministic=deterministic, render=False,\n",
        "                                 return_episode_rewards=True)\n",
        "        return rewards, steps\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "    def predict_proba(self, state):\n",
        "        '''\n",
        "        Predict the probability of each action given a state\n",
        "        https://stackoverflow.com/questions/66428307/how-to-get-action-propability-in-stable-baselines-3/70012691#70012691?newreg=bd5479b970664069b359903e0151b4a1\n",
        "        '''\n",
        "        model = self.model\n",
        "        obs = obs_as_tensor(state, model.policy.device)\n",
        "        dis = model.policy.get_distribution(obs)\n",
        "        probs = dis.distribution.probs\n",
        "        probs_np = probs.detach().numpy()\n",
        "        return probs_np\n",
        "\n",
        "    #############\n",
        "    # functions for making plots & videos\n",
        "\n",
        "    def make_video_frames(self, deterministic=False):\n",
        "        '''\n",
        "        For each step, plot obs & rendered screen in one figure for making videoes\n",
        "        '''\n",
        "        state = self.env.reset()\n",
        "        done = False\n",
        "        score = [0]\n",
        "        #self._make_combined_plot2(state, score, prob_actions)\n",
        "        #self._make_combined_plot(state, score)\n",
        "\n",
        "\n",
        "        while not done:\n",
        "        #for i in range(1):\n",
        "            prob_actions = self.predict_proba(state)\n",
        "            action, _ = self.model.predict(state, deterministic=deterministic)\n",
        "            state, reward, done, info = self.env.step(action)\n",
        "            score += reward\n",
        "            self._make_combined_plot2(state, score, prob_actions)\n",
        "            #self._make_combined_plot(state, score)\n",
        "\n",
        "\n",
        "    def _make_combined_plot2(self, state, score, prob_actions):\n",
        "        '''\n",
        "        Originally made for n_stack = 4 & n_skip = 4, SIMPLE_MOVEMENT\n",
        "        '''\n",
        "        # get rendered screen\n",
        "        im_render = self.env.render(mode=\"rgb_array\")\n",
        "\n",
        "        n_stack = state.shape[-1]\n",
        "        cmap = colors.ListedColormap(['red', 'skyblue', 'brown', 'blue'])\n",
        "        bounds = [-1.5, -0.5, 0.5, 1.5, 2.5]\n",
        "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "        #obs_loc = [[0, 1], [0, 2], [1, 1], [1, 2]]\n",
        "        obs_loc = [[0, 1], [1, 1], [2, 1], [3, 1]]\n",
        "        obs_text = ['t (current frame)', 't-4', 't-8', 't-12']\n",
        "        action_list = ['NOOP', 'right', 'right+A', 'right+B', 'right+A+B', 'A', 'left']\n",
        "\n",
        "\n",
        "        ##########\n",
        "        fig = plt.figure(dpi=100, figsize=(6, 6), constrained_layout=False, tight_layout=True)\n",
        "        gs = fig.add_gridspec(4, 2, width_ratios=[3, 1])\n",
        "\n",
        "        # individual obs frames\n",
        "        for n in range(n_stack):\n",
        "            ax = fig.add_subplot(gs[obs_loc[n][0], obs_loc[n][1]])\n",
        "            im = ax.imshow(state[0,:,:,n], cmap=cmap, norm=norm)\n",
        "            ax.set_axis_off()\n",
        "            ax.text(-0.5, 14.5, obs_text[n])\n",
        "\n",
        "        # prob_actions\n",
        "        ax = fig.add_subplot(gs[3, 0])\n",
        "        ax.bar(action_list, prob_actions[0])\n",
        "        plt.xticks(rotation=45)\n",
        "        ax.set_ylim(0, 1.05)\n",
        "\n",
        "        # rendered screen\n",
        "        ax = fig.add_subplot(gs[0:3, 0])\n",
        "        im = ax.imshow(im_render)\n",
        "        ax.set_axis_off()\n",
        "        ax.text(0, -5, 'score: '+str(int(score[0])))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "\n",
        "    def _make_combined_plot(self, state, score):\n",
        "        # get rendered screen\n",
        "        im_render = self.env.render(mode=\"rgb_array\")\n",
        "        n_stack = state.shape[-1]\n",
        "\n",
        "        cmap = colors.ListedColormap(['red', 'skyblue', 'brown', 'blue'])\n",
        "        bounds = [-1.5, -0.5, 0.5, 1.5, 2.5]\n",
        "        norm = colors.BoundaryNorm(bounds, cmap.N)\n",
        "\n",
        "        #obs_text = ['t (current frame)', 't-4', 't-8', 't-12']\n",
        "\n",
        "        fig = plt.figure(dpi=100, figsize=(5.5, 4), constrained_layout=False, tight_layout=True)\n",
        "        gs = fig.add_gridspec(4, 2, width_ratios=[4, 1])\n",
        "\n",
        "        # individual obs frames\n",
        "        for n in range(n_stack):\n",
        "            ax = fig.add_subplot(gs[n, 1])\n",
        "            im = ax.imshow(state[0,:,:,n], cmap=cmap, norm=norm)\n",
        "            ax.set_axis_off()\n",
        "\n",
        "        # rendered screen\n",
        "        ax = fig.add_subplot(gs[:, 0])\n",
        "        im = ax.imshow(im_render)\n",
        "        ax.set_axis_off()\n",
        "        ax.text(0, -5, 'score: '+str(int(score[0])))\n",
        "\n",
        "        plt.show()\n",
        "\n",
        "    def make_animation(self, deterministic=True, filename='gym_animation.gif', RETURN_FRAMES=False):\n",
        "        '''\n",
        "        Make an animation of the rendered screen\n",
        "        '''\n",
        "        # run policy\n",
        "        frames = []\n",
        "        states = self.env.reset()\n",
        "        done = False\n",
        "\n",
        "        while not done:\n",
        "            #frames.append(self.env.render(mode=\"rgb_array\"))\n",
        "            im = self.env.render(mode=\"rgb_array\")\n",
        "            frames.append(im.copy())\n",
        "            action, _ = self.model.predict(states, deterministic=deterministic)\n",
        "            states, reward, done, info = self.env.step(action)\n",
        "\n",
        "        if RETURN_FRAMES == False:\n",
        "            # make animation\n",
        "            imageio.mimsave(filename, frames, fps=50)\n",
        "        else: # make animation manually in case Mario gets stuck in the level and drags the animation for too long\n",
        "            return frames"
      ],
      "metadata": {
        "id": "hjW-rZKAnk8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class smb_grid:\n",
        "\n",
        "    def __init__(self, env):\n",
        "        self.ram = env.unwrapped.ram\n",
        "        self.screen_size_x = 16    # rendered screen size\n",
        "        self.screen_size_y = 13\n",
        "\n",
        "        self.mario_level_x = self.ram[0x6d]*256 + self.ram[0x86]\n",
        "        self.mario_x = self.ram[0x3ad]  # mario's position on the rendered screen\n",
        "        self.mario_y = self.ram[0x3b8] + 16 # top edge of (big) mario\n",
        "\n",
        "        self.x_start = self.mario_level_x - self.mario_x # left edge pixel of the rendered screen in level\n",
        "        self.rendered_screen = self.get_rendered_screen()\n",
        "\n",
        "\n",
        "    ########\n",
        "    # get background tile grid\n",
        "\n",
        "    def tile_loc_to_ram_address(self, x, y):\n",
        "        '''\n",
        "        convert (x, y) in Current tile (32x13, stored as 16x26 in ram) to ram address\n",
        "        x: 0 to 31\n",
        "        y: 0 to 12\n",
        "        '''\n",
        "        page = x // 16\n",
        "        x_loc = x%16\n",
        "        y_loc = page*13 + y\n",
        "\n",
        "        address = 0x500 + x_loc + y_loc*16\n",
        "\n",
        "        return address\n",
        "\n",
        "    def get_rendered_screen(self):\n",
        "        '''\n",
        "        Get the rendered screen (16 x 13) from ram\n",
        "        empty: 0\n",
        "        tile: 1\n",
        "        enemy: -1\n",
        "        mario: 2\n",
        "        '''\n",
        "\n",
        "        # Get background tiles\n",
        "\n",
        "        rendered_screen = np.zeros((self.screen_size_y, self.screen_size_x))\n",
        "        screen_start = int(np.rint(self.x_start / 16))\n",
        "\n",
        "        for i in range(self.screen_size_x):\n",
        "            for j in range(self.screen_size_y):\n",
        "                x_loc = (screen_start + i) % (self.screen_size_x * 2)\n",
        "                y_loc = j\n",
        "                address = self.tile_loc_to_ram_address(x_loc, y_loc)\n",
        "                #bg_screen2[j, i] = env.unwrapped.ram[address]\n",
        "\n",
        "                # Convert all types of tile to 1\n",
        "                if self.ram[address] != 0:\n",
        "                    rendered_screen[j, i] = 1\n",
        "\n",
        "        # Add mario\n",
        "        x_loc = (self.mario_x + 8) // 16\n",
        "        y_loc = (self.mario_y - 32) // 16 # top 2 rows in the rendered screen aren't stored in ram\n",
        "        if x_loc < 16 and y_loc < 13:\n",
        "            rendered_screen[y_loc, x_loc] = 2\n",
        "\n",
        "        # Add enemies\n",
        "        for i in range(5):\n",
        "            # check if the enemy is drawn\n",
        "            if self.ram[0xF + i] == 1:\n",
        "                enemy_x = self.ram[0x6e + i]*256 + self.ram[0x87 + i] - self.x_start\n",
        "                enemy_y = self.ram[0xcf + i]\n",
        "                x_loc = (enemy_x + 8) // 16\n",
        "                y_loc = (enemy_y + 8 - 32) // 16\n",
        "\n",
        "                # check if the enemy is inside the rendered screen\n",
        "                # 8/6/22 fixed bug where enemy with x_loc < 0 still got added to rendered_screen; doesn't seem to affect trained models' performance\n",
        "                # if x_loc < 16 and y_loc < 13:\n",
        "                if 0 <= x_loc < 16 and 0 <= y_loc < 13:\n",
        "                    rendered_screen[y_loc, x_loc] = -1\n",
        "\n",
        "        return rendered_screen"
      ],
      "metadata": {
        "id": "T6sPFobmno0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SMBRamWrapper(gym.ObservationWrapper):\n",
        "    def __init__(self, env, crop_dim=[0, 16, 0, 13], n_stack=4, n_skip=2):\n",
        "        '''\n",
        "        crop_dim: [x0, x1, y0, y1]\n",
        "        obs shape = (height, width, n_stack), n_stack=0 is the most recent frame\n",
        "        n_skip: e.g. n_stack=4, n_skip=2, use frames [0, 2, 4, 6]\n",
        "        '''\n",
        "        gym.Wrapper.__init__(self, env)\n",
        "        self.crop_dim = crop_dim\n",
        "        self.n_stack = n_stack\n",
        "        self.n_skip = n_skip\n",
        "        # Modified from stable_baselines3.common.atari_wrappers.WarpFrame()\n",
        "        # https://stable-baselines3.readthedocs.io/en/master/_modules/stable_baselines3/common/atari_wrappers.html#AtariWrapper\n",
        "        self.width = crop_dim[1] - crop_dim[0]\n",
        "        self.height = crop_dim[3] - crop_dim[2]\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=-1, high=2, shape=(self.height, self.width, self.n_stack), dtype=int\n",
        "        )\n",
        "\n",
        "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1))\n",
        "        #self.INDEX_SKIP = 1\n",
        "\n",
        "    def observation(self, obs):\n",
        "        grid = smb_grid(self.env)\n",
        "        frame = grid.rendered_screen # 2d array\n",
        "        frame = self.crop_obs(frame)\n",
        "\n",
        "        self.frame_stack[:,:,1:] = self.frame_stack[:,:,:-1] # shift frame_stack by 1\n",
        "        self.frame_stack[:,:,0] = frame # add current frame to stack\n",
        "        obs = self.frame_stack[:,:,::self.n_skip]\n",
        "        return obs\n",
        "\n",
        "    def reset(self):\n",
        "        obs = self.env.reset()\n",
        "        self.frame_stack = np.zeros((self.height, self.width, (self.n_stack-1)*self.n_skip+1))\n",
        "        grid = smb_grid(self.env)\n",
        "        frame = grid.rendered_screen # 2d array\n",
        "        frame = self.crop_obs(frame)\n",
        "        for i in range(self.frame_stack.shape[-1]):\n",
        "            self.frame_stack[:,:,i] = frame\n",
        "        obs = self.frame_stack[:,:,::self.n_skip]\n",
        "        return obs\n",
        "\n",
        "    def crop_obs(self, im):\n",
        "        '''\n",
        "        Crop observed frame image to reduce input size\n",
        "        Returns cropped_frame = original_frame[y0:y1, x0:x1]\n",
        "        '''\n",
        "        [x0, x1, y0, y1] = self.crop_dim\n",
        "        im_crop = im[y0:y1, x0:x1]\n",
        "        return im_crop\n"
      ],
      "metadata": {
        "id": "R6SnxBkYnt8l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lpPZKCHVI8xC"
      },
      "outputs": [],
      "source": [
        "class MarioNet(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.online = nn.Sequential(\n",
        "\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, output_dim),\n",
        "        )\n",
        "\n",
        "        self.target = copy.deepcopy(self.online)\n",
        "\n",
        "        # Q_target parameters are frozen.\n",
        "        for p in self.target.parameters():\n",
        "            p.requires_grad = False\n",
        "\n",
        "    def forward(self, input, model):\n",
        "        if model == \"online\":\n",
        "            return self.online(input)\n",
        "        elif model == \"target\":\n",
        "            return self.target(input)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class World:\n",
        "    def __init__(self, env):\n",
        "        self.env = env\n",
        "        self.mario_pos = 0\n",
        "        self.enemy_pos = []\n",
        "        self.episodes = 10000\n",
        "        self.episolon = 1\n",
        "        self.episilon_rate = 1/self.episodes\n",
        "        self.Q_table = {}\n",
        "        self.store_state = []\n",
        "        self.rng = np.random.default_rng()\n",
        "        self.total_reward = []\n",
        "\n",
        "    def update_epsilon(self, episode):\n",
        "        self.episolon = max(0.1, self.episolon - episode * self.episilon_rate)\n",
        "\n",
        "    def find_mario(self, state):\n",
        "        mask = (state == 2)\n",
        "        if len(np.argwhere(mask)) == 0:\n",
        "            return mask\n",
        "        return np.argwhere(mask)[0]\n",
        "\n",
        "    def find_enemy(self, state):\n",
        "        mask = (state == -1)\n",
        "        return np.argwhere(mask)\n",
        "\n",
        "\n",
        "    def check_ground(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        if mario_pos[0] + 1 == 13:\n",
        "            return False\n",
        "        return (state[mario_pos[0] + 1, mario_pos[1]] == 1)\n",
        "\n",
        "    def obstacle_ahead(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        return state[mario_pos[0] - 1: mario_pos[0] + 2, mario_pos[1] + 1]\n",
        "\n",
        "    def obstacle_near(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        return state[mario_pos[0] - 1: mario_pos[0] + 2, mario_pos[1] + 2]\n",
        "\n",
        "    def can_jump(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        return (state[mario_pos[0] - 1, mario_pos[1]] != 1) and (state[mario_pos[0] - 2,mario_pos[1]] != 1)\n",
        "\n",
        "    def enemy_near(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        enemies = self.find_enemy(state)\n",
        "        max_dist = 0\n",
        "        if len(enemies) == 0:\n",
        "            return False\n",
        "        for enemy in enemies:\n",
        "            if enemy[1] - mario_pos[1] < 0: continue\n",
        "            dist = max(abs(enemy[0] - mario_pos[0]), enemy[1] - mario_pos[1])\n",
        "            max_dist = max(max_dist, dist)\n",
        "        return max_dist <= 2\n",
        "\n",
        "    def enemy_bellow(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        if state[mario_pos[0] + 1, mario_pos[1]] == -1:\n",
        "            return True\n",
        "        return False\n",
        "\n",
        "    def enemy_mid(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        enemies = self.find_enemy(state)\n",
        "        max_dist = 0\n",
        "        if len(enemies) == 0:\n",
        "            return False\n",
        "        for enemy in enemies:\n",
        "            if enemy[1] - mario_pos[1] < 0: continue\n",
        "            dist = max(abs(enemy[0] - mario_pos[0]), enemy[1] - mario_pos[1])\n",
        "            max_dist = max(max_dist, dist)\n",
        "        return max_dist <= 3\n",
        "\n",
        "    def enemy_far(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        enemies = self.find_enemy(state)\n",
        "        max_dist = 0\n",
        "        if len(enemies) == 0:\n",
        "            return False\n",
        "        for enemy in enemies:\n",
        "            if enemy[1] - mario_pos[1] < 0: continue\n",
        "            dist = max(abs(enemy[0] - mario_pos[0]), enemy[1] - mario_pos[1])\n",
        "            max_dist = max(max_dist, dist)\n",
        "        return max_dist <= 4\n",
        "\n",
        "    def colision(self, state):\n",
        "        mario_pos = self.find_mario(state)\n",
        "        enemies = self.find_enemy(state)\n",
        "        colision = False\n",
        "        for enemy in enemies:\n",
        "            if (mario_pos[0] - enemy[0] == 1) or (enemy[1] - mario_pos[1] == 1):\n",
        "                colision = True\n",
        "\n",
        "        return colision\n",
        "\n",
        "\n",
        "    def get_state_action_tuple(self, state):\n",
        "        ground = self.check_ground(state)\n",
        "        jump = self.can_jump(state)\n",
        "        collision = self.colision(state)\n",
        "        near = self.enemy_near(state)\n",
        "        mid = self.enemy_mid(state)\n",
        "        far = self.enemy_far(state)\n",
        "        obstacle = self.obstacle_ahead(state)\n",
        "        enemy_bellow = self.enemy_bellow(state)\n",
        "        obstacle_near = self.obstacle_near(state)\n",
        "        return tuple([ground, jump, collision, near, mid, far, obstacle[0], obstacle[1], obstacle[2], enemy_bellow,\n",
        "                       obstacle_near[0], obstacle_near[1], obstacle_near[2]])\n",
        "\n",
        "    def get_states_action_tuple(self, states):\n",
        "        tp = tuple([])\n",
        "        for i in range(states.shape[2]):\n",
        "            tp += self.get_state_action_tuple(states[:,:,i])\n",
        "        return np.asarray(tp)\n",
        "\n",
        "\n",
        "    def adjust_reward(self, state, reward, action):\n",
        "        enemies = self.find_enemy(state)\n",
        "        mario_pos = self.find_mario(state)\n",
        "        if action == 0 or mario_pos.shape[0] != 3:\n",
        "            return reward\n",
        "        if state[mario_pos[0] + 1, mario_pos[1] + 1, 0] == 0: #não penalizar por pular poço\n",
        "            return reward\n",
        "        if state[mario_pos[0], mario_pos[1] + 1, 0] == 1 or state[mario_pos[0], mario_pos[1] + 2, 0] == 1: #não penalisar por pular obstáculo\n",
        "            return reward\n",
        "        if len(enemies) == 0:\n",
        "            return - 1\n",
        "        for enemy in enemies:\n",
        "            if (enemy[1] - mario_pos[1]) > 2: #penalisar o mario por pular de forma desnecessária\n",
        "                return -1\n",
        "\n",
        "        return reward\n",
        "\n"
      ],
      "metadata": {
        "id": "p6Fjy2eMnSqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CpgfeazvJJon"
      },
      "outputs": [],
      "source": [
        "\n",
        "class Mario:\n",
        "    def __init__(self, state_dim, action_dim, save_dir):\n",
        "        self.state_dim = state_dim\n",
        "        self.action_dim = action_dim\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "        # Mario's DNN to predict the most optimal action - we implement this in the Learn section\n",
        "        self.net = MarioNet(self.state_dim, self.action_dim).float()\n",
        "        self.net = self.net.to(device=self.device)\n",
        "\n",
        "        self.exploration_rate = 1\n",
        "        self.exploration_rate_decay = 0.99999975\n",
        "        self.exploration_rate_min = 0.1\n",
        "        self.curr_step = 0\n",
        "\n",
        "        self.save_every = 5e5  # no. of experiences between saving Mario Net\n",
        "        self.memory = TensorDictReplayBuffer(storage=LazyMemmapStorage(100000, device=torch.device(\"cpu\")))\n",
        "        self.batch_size = 32\n",
        "        self.gamma = 0.9\n",
        "        self.optimizer = torch.optim.Adam(self.net.parameters(), lr=0.00025)\n",
        "        self.loss_fn = torch.nn.SmoothL1Loss()\n",
        "        self.burnin = 1e4  # min. experiences before training\n",
        "        self.learn_every = 3  # no. of experiences between updates to Q_online\n",
        "        self.sync_every = 1e4  # no. of experiences between Q_target & Q_online sync\n",
        "\n",
        "    def act(self, state):\n",
        "\n",
        "        # EXPLORE\n",
        "        if np.random.rand() < self.exploration_rate:\n",
        "            action_idx = np.random.randint(self.action_dim)\n",
        "\n",
        "        # EXPLOIT\n",
        "        else:\n",
        "            state = state[0].__array__() if isinstance(state, tuple) else state.__array__()\n",
        "            state = torch.tensor(state, device=self.device).unsqueeze(0)\n",
        "            state = state.to(torch.float)\n",
        "            action_values = self.net(state, model=\"online\")\n",
        "            action_idx = torch.argmax(action_values, axis=1).item()\n",
        "\n",
        "        # decrease exploration_rate\n",
        "        self.exploration_rate *= self.exploration_rate_decay\n",
        "        self.exploration_rate = max(self.exploration_rate_min, self.exploration_rate)\n",
        "\n",
        "        # increment step\n",
        "        self.curr_step += 1\n",
        "        return action_idx\n",
        "\n",
        "\n",
        "    def cache(self, state, next_state, action, reward, done):\n",
        "        def first_if_tuple(x):\n",
        "            return x[0] if isinstance(x, tuple) else x\n",
        "        state = first_if_tuple(state).__array__()\n",
        "        next_state = first_if_tuple(next_state).__array__()\n",
        "\n",
        "        state = torch.tensor(state)\n",
        "        next_state = torch.tensor(next_state)\n",
        "        action = torch.tensor([action])\n",
        "        reward = torch.tensor([reward])\n",
        "        done = torch.tensor([done])\n",
        "\n",
        "        self.memory.add(TensorDict({\"state\": state, \"next_state\": next_state, \"action\": action, \"reward\": reward, \"done\": done}, batch_size=[]))\n",
        "\n",
        "    def recall(self):\n",
        "        batch = self.memory.sample(self.batch_size).to(self.device)\n",
        "        state, next_state, action, reward, done = (batch.get(key) for key in (\"state\", \"next_state\", \"action\", \"reward\", \"done\"))\n",
        "        return state, next_state, action.squeeze(), reward.squeeze(), done.squeeze()\n",
        "\n",
        "    def td_estimate(self, state, action):\n",
        "        state = state.to(self.device)\n",
        "        state = state.to(torch.float)\n",
        "        action = action.to(self.device)\n",
        "        current_Q = self.net(state, model=\"online\")[\n",
        "            np.arange(0, self.batch_size), action\n",
        "        ]  # Q_online(s,a)\n",
        "        return current_Q\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def td_target(self, reward, next_state, done):\n",
        "        next_state = next_state.to(torch.float)\n",
        "        next_state_Q = self.net(next_state, model=\"online\")\n",
        "        best_action = torch.argmax(next_state_Q, axis=1)\n",
        "        next_Q = self.net(next_state, model=\"target\")[\n",
        "            np.arange(0, self.batch_size), best_action\n",
        "        ]\n",
        "        return (reward + (1 - done.float()) * self.gamma * next_Q).float()\n",
        "\n",
        "    def update_Q_online(self, td_estimate, td_target):\n",
        "        loss = self.loss_fn(td_estimate, td_target)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "        return loss.item()\n",
        "\n",
        "    def sync_Q_target(self):\n",
        "        self.net.target.load_state_dict(self.net.online.state_dict())\n",
        "\n",
        "    def save(self):\n",
        "        save_path = (\n",
        "            self.save_dir / f\"mario_net_{int(self.curr_step // self.save_every)}.chkpt\"\n",
        "        )\n",
        "        torch.save(\n",
        "            dict(model=self.net.state_dict(), exploration_rate=self.exploration_rate),\n",
        "            save_path,\n",
        "        )\n",
        "        print(f\"MarioNet saved to {save_path} at step {self.curr_step}\")\n",
        "\n",
        "\n",
        "    def learn(self):\n",
        "        if self.curr_step % self.sync_every == 0:\n",
        "            self.sync_Q_target()\n",
        "\n",
        "        if self.curr_step % self.save_every == 0:\n",
        "            self.save()\n",
        "\n",
        "        if self.curr_step < self.burnin:\n",
        "            return None, None\n",
        "\n",
        "        if self.curr_step % self.learn_every != 0:\n",
        "            return None, None\n",
        "        # Sample from memory\n",
        "        state, next_state, action, reward, done = self.recall()\n",
        "\n",
        "        # Get TD Estimate\n",
        "        td_est = self.td_estimate(state, action)\n",
        "\n",
        "        # Get TD Target\n",
        "        td_tgt = self.td_target(reward, next_state, done)\n",
        "\n",
        "        # Backpropagate loss through Q_online\n",
        "        loss = self.update_Q_online(td_est, td_tgt)\n",
        "\n",
        "\n",
        "        return (td_est.mean().item(), loss)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Game:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.x0 = 0\n",
        "        self.x1 = 16\n",
        "        self.y0 = 0\n",
        "        self.y1 = 13\n",
        "        self.n_stack = 4\n",
        "        self.n_skip = 4\n",
        "        self.alfa = 0.5\n",
        "        self.gamma = 0.7\n",
        "        self.Episodes = 2000\n",
        "        env = gym_super_mario_bros.make('SuperMarioBros-1-1-v1')\n",
        "        env = JoypadSpace(env, [[\"right\"], [\"right\", \"A\"]])\n",
        "        self.env_wrap = SMBRamWrapper(env, [self.x0, self.x1, self.y0, self.y1], n_stack=self.n_stack, n_skip=self.n_skip)\n",
        "        self.world = World(env)\n",
        "        save_dir = Path(\"checkpoints\")\n",
        "        save_dir.mkdir(parents=True, exist_ok=True)\n",
        "        self.mario = Mario(52, 2, save_dir)\n",
        "\n",
        "\n",
        "    def update_q_table(self, target, states, action):\n",
        "        if self.world.check_dict(self.world.get_states_action_tuple(states, action)):\n",
        "            old_value = self.world.Q_table[self.world.get_states_action_tuple(states, action)]\n",
        "            self.world.Q_table[self.world.get_states_action_tuple(states, action)] = (1 - self.alfa) * old_value + target\n",
        "        else:\n",
        "            self.world.store_state.append(self.world.get_states_action_tuple(states, action))\n",
        "            self.world.Q_table.update({self.world.get_states_action_tuple(states, action): target})\n",
        "\n",
        "    def adjust_reward(self, state, reward, action, repeat_pos, info, done):\n",
        "        reward = self.world.adjust_reward(state, reward, action)\n",
        "        if repeat_pos[info['x_pos']] > 300:\n",
        "            reward -= 15\n",
        "            done = True\n",
        "        return done\n",
        "\n",
        "    def check_mario(self, states):\n",
        "        for i in range(states.shape[2]):\n",
        "            if len(np.argwhere(self.world.find_mario(states[:,:,i]))) == 0 or self.world.find_mario(states[:,:,i]).shape[0] > 3:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def play(self):\n",
        "        done = False\n",
        "        states = self.env_wrap.reset()\n",
        "        total_reward = 0\n",
        "        repeat_pos = np.zeros(4000)\n",
        "        max_pos = 0\n",
        "        while(not done):\n",
        "            if self.check_mario(states) is False:\n",
        "                break\n",
        "            state = self.world.get_states_action_tuple(states)\n",
        "            action = self.mario.act(state)\n",
        "            new_states, reward, done, info = self.env_wrap.step(action)\n",
        "            if self.check_mario(new_states) is False:\n",
        "                break\n",
        "            done = self.adjust_reward(states[:,:,0],reward,action,repeat_pos,info, done)\n",
        "            new_state = self.world.get_states_action_tuple(new_states)\n",
        "            self.mario.cache(state, new_state, action, reward, done)\n",
        "            q, loss = self.mario.learn()\n",
        "            states = new_states\n",
        "            repeat_pos[info['x_pos']] += 1\n",
        "            total_reward += reward\n",
        "            max_pos = info['x_pos']\n",
        "            if done or info[\"flag_get\"]:\n",
        "                break\n",
        "        return total_reward, max_pos\n",
        "\n",
        "    def run(self):\n",
        "        with tqdm(total= self.Episodes) as pbar:\n",
        "            for episode in range(self.Episodes):\n",
        "                total_reward, max_pos = self.play()\n",
        "                self.world.update_epsilon(episode)\n",
        "                self.world.total_reward.append(total_reward)\n",
        "                if episode % 100 == 0:\n",
        "                    print(f'Epoca {episode + 1}: {total_reward}')\n",
        "                    print(max_pos)\n",
        "                pbar.update(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "SzrFnG7ipcTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_i39ksfOJRaF",
        "outputId": "98267dca-659c-4654-d153-7ec23b8d6d95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:593: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-1-1-v1 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
            "  logger.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "  0%|          | 0/2000 [00:00<?, ?it/s]/usr/local/lib/python3.10/dist-packages/gym/utils/passive_env_checker.py:227: DeprecationWarning: \u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n",
            "  logger.deprecation(\n",
            "  0%|          | 1/2000 [00:00<29:36,  1.13it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  5%|▌         | 101/2000 [05:40<1:43:52,  3.28s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 101: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 201/2000 [11:30<1:18:32,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 201: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 15%|█▌        | 301/2000 [17:23<1:22:40,  2.92s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 301: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 20%|██        | 401/2000 [23:10<1:27:16,  3.27s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 401: 523.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 501/2000 [29:27<1:05:21,  2.62s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 501: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 30%|███       | 601/2000 [34:52<1:21:31,  3.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 601: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 35%|███▌      | 701/2000 [40:22<1:17:52,  3.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 701: 517.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 40%|████      | 801/2000 [47:02<1:17:12,  3.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 801: 519.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 45%|████▌     | 901/2000 [53:39<1:02:16,  3.40s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 901: 523.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 48%|████▊     | 952/2000 [56:44<50:01,  2.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MarioNet saved to checkpoints/mario_net_1.chkpt at step 500000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 50%|█████     | 1001/2000 [59:42<1:10:04,  4.21s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1001: 521.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 55%|█████▌    | 1101/2000 [1:05:38<36:06,  2.41s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1101: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 60%|██████    | 1201/2000 [1:11:42<38:19,  2.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1201: 519.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 65%|██████▌   | 1301/2000 [1:17:52<42:02,  3.61s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1301: 518.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 70%|███████   | 1401/2000 [1:24:13<47:25,  4.75s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1401: 523.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 75%|███████▌  | 1501/2000 [1:30:57<38:53,  4.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1501: 516.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 80%|████████  | 1601/2000 [1:36:58<25:52,  3.89s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1601: 520.0\n",
            "594\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 85%|████████▌ | 1701/2000 [1:42:58<07:39,  1.54s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1701: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 90%|█████████ | 1801/2000 [1:48:40<15:23,  4.64s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1801: 584.0\n",
            "654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▍| 1894/2000 [1:54:14<08:05,  4.58s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MarioNet saved to checkpoints/mario_net_2.chkpt at step 1000000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 95%|█████████▌| 1901/2000 [1:54:45<05:55,  3.59s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoca 1901: 231.0\n",
            "296\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2000/2000 [2:01:02<00:00,  3.63s/it]\n"
          ]
        }
      ],
      "source": [
        "game = Game()\n",
        "game.run()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%pdb off"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1culvjbxffZL",
        "outputId": "99903a42-bee1-48c9-90bf-08ca5f00014f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatic pdb calling has been turned OFF\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}